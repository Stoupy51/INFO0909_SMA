
# Imports
from config import *
import stouputils as stp
import requests
import json
from autogen_core import MessageContext, BaseAgent
from src.reputation import Reputation
import pandas as pd

# Class
class Ollama(BaseAgent):
    def __init__(self) -> None:
        super().__init__(self.__class__.__name__)
        self.msg: Message = Message(origin=self.__class__.__name__)
        self.reputation: Reputation = Reputation()
        
        # Load dataset for reputation initialization
        data: pd.DataFrame = pd.read_csv(DATASET)
        data["generated"] = data["generated"].astype(int)
        
        # Initialize reputation with this data
        stp.info("Initializing Ollama reputation...")
        for _, row in data.iterrows():
            text = row[0]
            true_class = "ai" if row[1] == 1 else "human"
            
            # Request server for classification
            data_req: dict = {
                "model": "llama3.2",
                "prompt": f"Can you simply answer 'Yes' or 'No' if the following sentence has been generated by an AI?\n\n{text}",
                "stream": False,
            }
            
            try:
                # Request server
                response = requests.post("http://localhost:11434/api/generate", json=data_req)
                
                if response.status_code == 200:
                    response_data: dict = response.json()
                    label: str = response_data["response"].lower()
                    label = "".join(x for x in label if x in "abcedfghijklmnopqrstuvwxyz")
                    predicted_class = "ai" if label in ["yes", "oui"] else "human"
                    
                    # Update reputation
                    self.reputation.update(predicted_class == true_class)
                
            except Exception as e:
                stp.error(f"Error during Ollama reputation initialization: {e}")
                continue
        
        stp.info(f"Ollama initial reputation (beta): {self.reputation.get_beta()}")
        
    async def on_message_impl(self, message: Message, ctx: MessageContext) -> None:
        """ Receive a message and send 1 if content is generated by AI, else 0 """
        stp.info(f"Ollama: Received message from {message.origin}")

        # Prepare data for the model
        content: str = str(message.content)
        data: dict = json.loads(message.data)
        vote: str = data.get("request")

        if vote == "majoritaire":
            prompt: str = "Can you simply answer 'Yes' or 'No' if the following sentence has been generated by an AI?"
        elif vote == "borda":
            prompt: str = "Can you classify if the text has been generated by an AI, simply give me the probabilities for each classes between 0 and 1, no explanation ? Answer like this: [human:proba, ai:proba]"
        elif vote == "paxos":
            phase = data.get("phase")
            if phase == "propose":
                prompt: str = "Can you classify if the text has been generated by an AI, simply give me the probabilities for each classes between 0 and 1, no explanation ? Answer like this: [human:proba, ai:proba]"
            else:
                # In voting phase, just return vote for highest beta candidate
                candidates = data.get("candidates", {})
                if candidates:
                    # Find candidate with highest beta
                    best_candidate = max(candidates.items(), key=lambda x: x[1][1])[0]
                    self.msg.content = json.dumps({"voted_for": best_candidate})
                    return

        # Request server for classification
        if vote in ["majoritaire", "borda"] or (vote == "paxos" and phase == "propose"):
            data_req: dict = {
                "model": "llama3.2",
                "prompt": f"{prompt}\n\n{content}",
                "stream": False,
            }
            
            # Request server
            response = requests.post("http://localhost:11434/api/generate", json=data_req)

            # Vérifier la réponse
            if response.status_code == 200:
                response_data: dict = response.json()
                label: str = response_data["response"].lower()
            else:
                stp.error(f"Erreur lors de la requête : {response.status_code}, {response.text}")
                label = "[human:0.5, ai:0.5]"  # Default fallback

            # If majoritaire
            if vote == "majoritaire":
                label = "".join(x for x in label if x in "abcedfghijklmnopqrstuvwxyz")
                # Get the label result
                labels: dict[str, str] = {"yes": "1", "no": "0", "oui": "1", "non": "0"}
                self.msg.content = labels[label]

            # If borda
            elif vote == "borda":
                # Extract probabilities from label string "[human:0.7, ai:0.3]" -> ["human:0.7", "ai:0.3"]
                probas: list[str] = label[1:-1].split(',')
                
                # Parse probability values for each class
                proba_human: float = float(probas[0].split(':')[1])  # Get human probability
                proba_ai: float = float(probas[1].split(':')[1])     # Get AI probability
                
                # Create list of tuples with class names and probabilities
                probas_tuple: list[tuple[str, float]] = [("human", proba_human), ("ai", proba_ai)]
                
                # Sort by probability in descending order
                probas_tuple.sort(key=lambda x: x[1], reverse=True)
                
                # Build Borda count string - highest probability gets most points
                text: str = ""
                nb_points: int = len(probas_tuple)  # Start with max points
                for candidat, proba in probas_tuple:
                    text += f"{candidat} {nb_points},"  # Add "class points," for each
                    nb_points -= 1                      # Decrease points for next class

                self.msg.content = text  # Set message content to Borda count string
            
            # If paxos propose phase
            elif vote == "paxos" and phase == "propose":
                # Extract probabilities like in borda
                probas: list[str] = label[1:-1].split(',')
                proba_human: float = float(probas[0].split(':')[1])
                proba_ai: float = float(probas[1].split(':')[1])
                
                # Determine class with highest probability
                if proba_human > proba_ai:
                    predicted_class = "human"
                    confidence = proba_human
                else:
                    predicted_class = "ai"
                    confidence = proba_ai
                
                # Send proposal with confidence
                self.msg.content = json.dumps({
                    "class": predicted_class,
                    "confidence": confidence,
                    "beta": self.reputation.get_beta()
                })

        # Send back
        await self.send_message(self.msg, ctx.sender)    # type: ignore
        

