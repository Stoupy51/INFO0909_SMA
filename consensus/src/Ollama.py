
# Imports
import pandas as pd
import numpy as np
import stouputils as stp
from config import *
from autogen_core import MessageContext, BaseAgent
import requests

# Class
class Ollama(BaseAgent):
    def __init__(self) -> None:
        super().__init__(self.__class__.__name__)
        self.msg: Message = Message(origin=self.__class__.__name__)
        
    async def on_message_impl(self, message: Message, ctx: MessageContext) -> None:
        """ Receive a message and send 1 if content is generated by AI, else 0 """
        stp.info(f"Ollama: Received message from {message.origin}")

        # Prepare data for the model
        content: str = str(message.content)
        vote: str = json.loads(message.data).get("request")

        if vote == "majoritaire":
            prompt: str = "Can you simply answer 'Yes' or 'No' if the following sentence has been generated by an AI?\n\n"


        data: dict = {
            "model": "llama3.2",	# https://ollama.com/download/OllamaSetup.exe
            "prompt": prompt + content,
            "stream": False,
        }
        
        # Request server
        response = requests.post("http://localhost:11434/api/generate", json=data)

        # Vérifier la réponse
        if response.status_code == 200:
            response_data: dict = response.json()
            label: str = response_data["response"].lower()
        else:
            stp.error(f"Erreur lors de la requête : {response.status_code}, {response.text}")

        if vote == "majoritaire":
            label = "".join(x for x in label if x in "abcedfghijklmnopqrstuvwxyz")
            # Get the label result
            labels: dict[str, str] = {"yes": "1", "no": "0", "oui": 1, "non": 0}
            msg = Message(labels[label])

        # Send back
        await self.send_message(msg, ctx.sender)
        

